# -*- coding: utf-8 -*-
"""Mathur_Garima_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-n5bkfCWGempJo8SfRlzdEBf8JRfOX-o

# Importing all Libraries :
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
from sklearn import metrics
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.model_selection import train_test_split
import math
import keras
from keras.models import Sequential
from tensorflow.keras import layers
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import *
from sklearn.metrics import roc_curve ,auc
import matplotlib.pyplot as plt

"""# Uploading the Dataset :"""

from google.colab import files
uploaded = files.upload()
import io
df = pd.read_csv(io.BytesIO(uploaded['diabetes.csv']))

"""# Dataset Preparation :"""

df.head()

df.info()

df.isnull().sum()

X=df.iloc[:,0:7]
Y=df.iloc[:,8]

X.head()

Y.head()

"""# Implementing the Calulate function to evaluate all parameters :"""

def calculate(confusion_matrix):
    l1={}
    True_positive=confusion_matrix[0,0]
    False_Positive=confusion_matrix[0,1]
    False_Negative=confusion_matrix[1,0]
    True_Negative=confusion_matrix[1,1]
    TP=True_positive
    FP=False_Positive
    FN=False_Negative
    TN=True_Negative
    True_positive_rate= TP/(TP + FN)
    True_negative_rate= TN/(TN + FP)
    False_positive_rate= FP/(TN + FP)
    False_negative_rate= FN/(TP + FN)
    Recall= TP/(TP + FN)
    Precision= TP/(TP + FP)
    F1_measure= (2 * TP)/(2 * TP + FP + FN)
    Accuracy=(TP + TN)/(TP + FP + FN + TN)
    Error_rate= (FP + FN)/(TP + FP + FN + TN)
    Balanced_Accuracy= 1/2 *((TP/(TP+FN) + (TN/(TN+FP))))
    True_Skill_Statistics = ((TP/(TP+FN))-(FP/(FP+TN)))
    Heidke_Skill_Score=2*((TP * TN) - (FP * FN))/((TP + FN)*(FN+TN)+(TP+FP)*(FP+TN))
    l1 = {'False negative':FN, 'False positive': FP, 'True positive': TP, 'True negative': TN,
              'True positive rate': True_positive_rate, 'True negative rate': True_negative_rate, 'False positive rate': False_positive_rate, 'False negative rate': False_negative_rate, 
              'Recall': Recall, 'Precision': Precision, 'F1 measure': F1_measure, 'Error rate': Error_rate,'Accuracy': Accuracy,'Balanced Accuracy':Balanced_Accuracy,'True Skill Statistics':True_Skill_Statistics,'Heidke Skill Score':Heidke_Skill_Score}
    result = pd.DataFrame([l1])
    return result

"""# Implementing Naive Bayes :"""

from sklearn.naive_bayes import GaussianNB
total_score = 0.0
for i in range(1,11):
    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)
    model1 = GaussianNB()
    model1.fit(X_train,y_train)
    prediction_value=model1.predict(X_test)
    confusion_matrix_naive=confusion_matrix(y_test, prediction_value)
    print("iteration :", i)
    metrics.plot_roc_curve(model1, X_test, y_test)
    plt.show()
    plt.show()
    final_value=calculate(confusion_matrix_naive)
    display(final_value)
    total_score = total_score + accuracy_score(prediction_value,y_test)
print(" Mean Accuracy of Naive Bayes :", (total_score/10)*100)

"""# Implementing LSTM :"""

for i in range(1,11):
    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)
    model2 = Sequential()
    model2.add(LSTM(units = 64, return_sequences = True,input_shape= (X_train.shape[1],1)))
    model2.add(LSTM(units = 64, return_sequences = True))
    model2.add(Dense(units = 1,activation='softmax'))
    model2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])
    model2.fit(X_train,y_train)
    prediction_rm=model2.predict(X_test)
    prediction_value=prediction_rm[:,0]
    print("iteration",i)
    y_pred_keras = prediction_value.ravel()
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)
    auc_keras = auc(fpr_keras, tpr_keras)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr_keras, tpr_keras, label='LSTM (area = {:.3f})'.format(auc_keras))
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve')
    plt.legend(loc='best')
    plt.show()
    n_lstm=tf.math.confusion_matrix(y_test,prediction_value)
    confusion_matrix_lstm=n_lstm.numpy()
    final_value=calculate(confusion_matrix_lstm)
    display(final_value)
print("Mean Accuracy of LSTM on data: ")
model2.evaluate(X_test,y_test)

"""# Implementing Random Forest :"""

from sklearn.ensemble import RandomForestClassifier
total_score = 0.0
for i in range(1,11):
    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)
    model3 = RandomForestClassifier()
    model3.fit(X_train,y_train)
    prediction_rm=model3.predict(X_test)
    print("iteration :", i)
    metrics.plot_roc_curve(model3, X_test, y_test)
    plt.show()
    confusion_matrix_Random=confusion_matrix(y_test, prediction_rm)
    final_value=calculate(confusion_matrix_Random)
    display(final_value)
    total_score = total_score + accuracy_score(prediction_rm,y_test)
print("Mean Accuracy of Random Forest on data: " , (total_score/10)*100)